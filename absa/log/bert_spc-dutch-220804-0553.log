cuda memory allocated: 436982272
> n_trainable_params: 109139715, n_nontrainable_params: 0
> training arguments:
>>> model_name: bert_spc
>>> dataset: dutch
>>> optimizer: <class 'torch.optim.adam.Adam'>
>>> initializer: <function xavier_uniform_ at 0x7f536ea3bb00>
>>> lr: 2e-05
>>> dropout: 0.1
>>> l2reg: 0.01
>>> num_epoch: 20
>>> batch_size: 16
>>> log_step: 10
>>> embed_dim: 300
>>> hidden_dim: 300
>>> bert_dim: 768
>>> pretrained_bert_name: GroNLP/bert-base-dutch-cased
>>> max_seq_len: 85
>>> polarities_dim: 3
>>> hops: 3
>>> patience: 5
>>> seed: 1234
>>> valset_ratio: 0
>>> model_class: <class 'models.bert_spc.BERT_SPC'>
>>> dataset_file: {'train': './datasets/dutch_data_train.xlsx.seg', 'test': './datasets/dutch_data_test.xlsx.seg'}
>>> inputs_cols: ['concat_bert_indices', 'concat_segments_indices']
>>> device: cuda
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 0
loss: 1.0034, acc: 0.5625
loss: 0.8824, acc: 0.6312
loss: 0.8107, acc: 0.6583
loss: 0.7869, acc: 0.6672
loss: 0.7603, acc: 0.6775
loss: 0.7475, acc: 0.6781
loss: 0.7268, acc: 0.6839
loss: 0.7173, acc: 0.6852
loss: 0.6974, acc: 0.6937
loss: 0.6836, acc: 0.6987
> val_acc: 0.9002, val_recall: 0.5638, val_f1: 0.5714
>> saved: state_dict/bert_spc_dutch_val_acc_0.9002
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 1
loss: 0.4919, acc: 0.8250
loss: 0.4421, acc: 0.8292
loss: 0.4058, acc: 0.8350
loss: 0.4075, acc: 0.8268
loss: 0.3995, acc: 0.8375
loss: 0.4131, acc: 0.8273
loss: 0.4223, acc: 0.8308
loss: 0.4269, acc: 0.8258
loss: 0.4183, acc: 0.8272
loss: 0.4336, acc: 0.8230
loss: 0.4333, acc: 0.8247
> val_acc: 0.9047, val_recall: 0.7291, val_f1: 0.6961
>> saved: state_dict/bert_spc_dutch_val_acc_0.9047
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 2
loss: 0.2541, acc: 0.9125
loss: 0.2873, acc: 0.8969
loss: 0.2891, acc: 0.8875
loss: 0.2889, acc: 0.8891
loss: 0.3100, acc: 0.8800
loss: 0.2957, acc: 0.8875
loss: 0.3055, acc: 0.8866
loss: 0.3122, acc: 0.8844
loss: 0.3205, acc: 0.8819
loss: 0.3163, acc: 0.8831

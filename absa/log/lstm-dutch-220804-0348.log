cuda memory allocated: 7343104
> n_trainable_params: 723303, n_nontrainable_params: 1112100
> training arguments:
>>> model_name: lstm
>>> dataset: dutch
>>> optimizer: <class 'torch.optim.adam.Adam'>
>>> initializer: <function xavier_uniform_ at 0x7f93d5094a70>
>>> lr: 2e-05
>>> dropout: 0.1
>>> l2reg: 0.01
>>> num_epoch: 50
>>> batch_size: 16
>>> log_step: 10
>>> embed_dim: 300
>>> hidden_dim: 300
>>> bert_dim: 768
>>> pretrained_bert_name: GroNLP/bert-base-dutch-cased
>>> max_seq_len: 85
>>> polarities_dim: 3
>>> hops: 3
>>> patience: 5
>>> seed: 1234
>>> valset_ratio: 0
>>> model_class: <class 'models.lstm.LSTM'>
>>> dataset_file: {'train': './datasets/dutch_data_train.xlsx.seg', 'test': './datasets/dutch_data_test.xlsx.seg'}
>>> inputs_cols: ['text_indices']
>>> device: cuda
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 0
loss: 0.9914, acc: 0.6375
loss: 0.9993, acc: 0.6156
loss: 0.9932, acc: 0.6229
loss: 0.9869, acc: 0.6297
loss: 0.9885, acc: 0.6225
loss: 0.9868, acc: 0.6219
loss: 0.9775, acc: 0.6348
loss: 0.9688, acc: 0.6453
loss: 0.9663, acc: 0.6458
loss: 0.9657, acc: 0.6438
